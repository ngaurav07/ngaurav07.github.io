---
title: "DAAL: Density-Aware Adaptive Line Margin Loss
for Multi-Modal Deep Metric Learning"
collection: publications
permalink: /publication/deep-metric-learning
excerpt: 'Multi-modal deep metric learning is crucial for effectively capturing diverse representations in tasks such as face verification, fine-grained object recognition, and product search. Traditional approaches to metric learning, whether based on distance or margin metrics, primarily emphasize class separation, often overlooking the intra-class distribution essential for multi-modal feature learning. In this context, we propose a novel loss function called Density-Aware Adaptive Margin Loss(DAAL), which preserves the density distribution of embeddings while encouraging the formation of adaptive sub-clusters within each class. By employing an adaptive line strategy, DAAL not only enhances intra-class variance but also ensures robust inter-class separation, facilitating effective multi-modal representation. Comprehensive experiments on benchmark fine-grained datasets demonstrate the superior performance of DAAL, underscoring its potential in advancing retrieval applications and multi-modal deep metric learning.'
date: 2024-10-09
venue: 'arXiv'
# slidesurl: 'http://academicpages.github.io/files/slides1.pdf'
paperurl: 'https://arxiv.org/abs/2410.05438'
# citation: 'Gaurav Neupane, You. (2009). &quot;Paper Title Number 1.&quot; <i>Journal 1</i>. 1(1).'
---

[View the Paper](https://arxiv.org/pdf/2410.05438)

Metric learning which focus on learning discriminative feature in high-dimensional space has been extensively studied in the past decades due to its broad range of applications, e.g.,
k-nearest neighbor classification, image retrieval  and
clustering. The main aim of metric learning is to create
an effective distance metric that groups samples of the same
class more closely together, while increasing the separation
between samples of different classes. Numerous algorithms
have been introduced to achieve this.
Early work in DML, referred to as classical DML methods,
relied on hand-crafted features to represent examples, with the
primary objective of learning a feature mapping that projects
these examples from the original feature space to a new,
more discriminative space. While these methods were effective
at deriving useful metrics from the input features, the use
of hand-crafted features often led to information loss, and
the high dimensionality of the input space posed significant
challenges. Although approaches like PCA were employed
to reduce the dimensionality , and later strategies were
introduced to lower computational costs , these issues
persisted.

With the rise of Deep Neural Networks (DNNs) and Deep
Convolutional Neural Networks (CNNs), researchers began
focusing on learning data embeddings directly through neural
networks ,eliminating the need for manual feature extraction. This method, known as Deep Metric Learning(DML),
has resulted in substantial performance improvements ,
opening a new frontier in the field. By leveraging the ability to
learn non-linear feature representations, deep metric learning
has demonstrated outstanding results across a wide range of
tasks, including highlight detection ,zero-shot image
classification ,clustering, image retrieval,visual product search,face recognition ,
, person re-identification ,feature matching ,
fine-grained image classification, zero-shot learning
,and collaborative filtering. Moreover, deep
metric learning has been successfully applied to 3D retrieval
tasks.
Loss functions are essential to deep metric learning. While
the traditional softmax loss is widely utilized, it often falls
short in generating highly discriminative feature vectors. This limitation prompted researchers to develop
improved loss functions that enhance intra-class compactness
and inter-class separability, ultimately boosting recognition accuracy and generalization. These efforts led to the emergence
of distance-based DML loss functions, particularly contrastive
loss (also known as the Siamese network),
. Although contrastive loss has demonstrated significant
success, it necessitates precise real-valued pairwise similarities
or distances in the training data, which are frequently unavailable in real-world applications. To address this issue, triplet
loss , was introduced by Weinberger and Saul. This
loss function encourages the features of data points with the
same identity to be closer together than those with different
identities by leveraging relative similarity among pairs. Triplet
loss has since been widely adopted in various tasks, including
image retrieval ,  and face recognition . Furthermore,
contributions such as N-pair loss  and various refinements
of triplet loss , have further enhanced the learning
of robust feature representations.

However, triplet loss introduces a challenge due to its requirement for triplet constraints, which can scale up to O(n3),where n represents the number of original examples—making
it computationally infeasible for large-scale datasets. And also
since most deep models are trained using Stochastic Gradient
Descent (SGD), which processes only mini-batches of data per
iteration, the triplet loss strategy struggles to capture the full
data distribution. The limited size of mini-batches constrains
the available information compared to the entire dataset.
To address this, algorithms need to devise effective sampling
strategies to construct mini-batches and sample triplet constraints from them. A straightforward approach is to increase
the mini-batch size, but this comes with challenges, such
as GPU memory limitations and increased complexity in
sampling triplets. An alternative method proposed by 
involves generating mini-batches from neighboring classes.
Additionally, various sampling strategies have been developed
to improve constraint selection: suggests sampling semihard negative examples,  uses all negative examples within a
margin for each positive pair,  introduces distance-weighted
sampling based on proximity to the anchor example, and 
selects hard triplets with a dynamic violation margin from
a hierarchical class-level tree. Despite these improvements,
all these strategies may still fail to accurately capture the
distribution of the entire dataset.
The current state-of-the-art in deep metric learning predominantly relies on margin-based softmax loss functions,
commonly referred to as margin-based loss functions. These
approaches enhance feature discrimination by introducing a
margin to each identity, thereby increasing class separation.
Prominent methods include SphereFace , which applies
a multiplicative angular margin, CosFace , which introduces an additive cosine margin, and ArcFace which incorporates an additive angular margin to further boost class separation. These techniques assume that all classes are
adequately represented with sufficient samples to accurately
describe their distributions, thus employing a constant margin
for all classes is deemed sufficient.
To address the issue of distribution, strategies have been
proposed to enlarge intra-class variation while penalizing the
overlap of representation distributions across different classes
in the embedding space. This approach aims to ensure that the
embedding representations are sufficiently spread out to fully
leverage the expressive power of the embedding space. Density
adaptability  focuses on preserving inter-class density by
maintaining the relationships between class densities in both
the original and embedded spaces, though this introduces additional computational complexity. SoftTriple Loss , which
maintains multiple centers, was also proposed to improve this
process. However, the former method is not applicable to
datasets without prior information, and the latter method may
inadvertently force points to cluster closely to sub-clusters
while ignoring continuous distribution.
As a result of the aforementioned challenges, our study
proposes a novel loss function designed to act as a regularizer capable of capturing continuous distributions without
forcing points into local clusters while maintaining inter-class
separability. This approach not only ensures clear separation between classes but also preserves the inherent distribution of
each cluster. We achieve this by implementing an ”elastic line”
mechanism, which adapts to the class distribution by mapping
it to an elastic line based on the variance of the class, allowing
for flexible representation of the data structure.
The contributions of this paper are as follows:
• Introduction and investigation of major loss functions in
deep metric learning, explaining how they differ from
the proposed DAAL (Density-Aware Elastic Line margin
loss).
• Proposal of a novel loss function termed DAAL, a
density-aware elastic line margin loss, demonstrating
superior state-of-the-art results compared to other alternatives.
• Demonstration of the ease of implementing DAAL in pretrained Convolutional Neural Networks (CNNs), enabling
straightforward integration into existing architectures.
The remainder of this paper is organized as follows: Section
II reviews related works on various types of deep metric
learning. Section III introduces our novel regularizer loss
function, along with an analysis and comparison to similar
loss functions. Section IV presents performance comparisons
on benchmark datasets. Finally, Section V concludes the paper
and discusses potential future research directions.